{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6cd5b91",
   "metadata": {},
   "source": [
    "## Библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e0ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "\n",
    "import tqdm\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "pd.options.display.float_format = '{:.10f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22385340",
   "metadata": {},
   "source": [
    "## Данные"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996c5946",
   "metadata": {},
   "source": [
    "Вычисляем размер групп под условия теста:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a54684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_size(alpha, beta, std_a, std_b, effect):\n",
    "    '''\n",
    "    Вычисляет количество наблюдений в каждой группе при заданных:\n",
    "    alpha, beta - ошибках 1 и 2 рода, \n",
    "    std_a, std_b - стандартных отколениях в контрольной и экспериментальной группах,\n",
    "    effect - минимальном ожидаемом эффекте\n",
    "    '''\n",
    "    norm_rv = stats.norm(loc=0, scale=1)\n",
    "    t_alpha = norm_rv.ppf(1 - alpha / 2)\n",
    "    t_beta = norm_rv.ppf(1 - beta)\n",
    "    var = (std_a**2) + (std_b)**2\n",
    "    sample_size = int((t_alpha + t_beta) ** 2 * var / (effect ** 2))\n",
    "    return sample_size\n",
    "\n",
    "alpha = 0.05\n",
    "beta = 0.2\n",
    "std_a = 800\n",
    "std_b = 800\n",
    "effect = 100\n",
    "\n",
    "sample_size = get_sample_size(alpha, beta, std_a, std_b, effect)\n",
    "sample_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97507ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000                           # общее количество пользователей в популяции\n",
    "w_one, w_two = 0.5, 0.5             # доли страт в популяции\n",
    "popul_size_one = int(N * w_one)              # количество пользователей первой страты\n",
    "popul_size_two = int(N * w_two)              # количество пользователей второй страты\n",
    "mu_one, mu_two = 2000, 3000         # средние значения метрики в стратах\n",
    "std_one, std_two = 625, 625         # стандартное отклонение метрики в стратах\n",
    "\n",
    "# ! Будем считать, что при объединении страт в контрольной и экспериментальной группах стандартное отклоение составит 800\n",
    "# (именно для такого стандартного отклонения мы вначале рассчитывали количество наблюдений в группах)\n",
    "    \n",
    "sample_size = get_sample_size(alpha, beta, std_a, std_b, effect) + 100 # возьмем количество наблюдений в группах с запасом \n",
    "                                                                       # на 100 больше, чем нужно\n",
    "sample_size_one = int(sample_size * w_one)\n",
    "sample_size_two = int(sample_size * w_two)\n",
    "\n",
    "strat_to_param = {1: (popul_size_one, sample_size_one, mu_one, std_one), 2: (popul_size_two, sample_size_two, mu_two, std_two)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec754b1",
   "metadata": {},
   "source": [
    "Функции для случайного и стратифицированного семплирования данных в базовый датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522f6254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_data(strat_to_param):\n",
    "    \"\"\"\n",
    "    Генерирует данные случайным семплированием:\n",
    "    Возвращает датафрейм со значениями метрики и номерами страт пользователей\n",
    "    в контрольной и экспериментальной группах:\n",
    "    strats - cписок с распределением страт в популяции,\n",
    "    sample_size - размеры групп,\n",
    "    strat_to_param - словарь с параметрами страт,\n",
    "    effect - размер эффекта\n",
    "    \"\"\"\n",
    "    \n",
    "    strats = [1 for _ in range(strat_to_param[1][0])] + \\\n",
    "             [2 for _ in range(strat_to_param[2][0])]\n",
    "    \n",
    "    control_strats, pilot_strats = np.random.choice(strats, (2, strat_to_param[2][1] + \\\n",
    "                                                                strat_to_param[2][1]), False)\n",
    "    \n",
    "    control, pilot = [], []\n",
    "    \n",
    "    for strat, (_, _, mu, std) in strat_to_param.items():\n",
    "        n_control = np.sum(control_strats == strat)\n",
    "        n_pilot = np.sum(pilot_strats == strat)\n",
    "        control += [(x, strat) for x in stats.norm(mu, std).rvs(n_control)]\n",
    "        pilot += [(x, strat) for x in stats.norm(mu, std).rvs(n_pilot)]\n",
    "        \n",
    "    control_df = pd.DataFrame(control, columns = ['metric', 'strat'])\n",
    "    control_df['is_treatment'] = 0\n",
    "    pilot_df = pd.DataFrame(pilot, columns = ['metric', 'strat'])\n",
    "    pilot_df['is_treatment'] = 1\n",
    "    \n",
    "    return pd.concat([control_df, pilot_df], axis = 0)\n",
    "\n",
    "def get_stratified_data(strat_to_param):\n",
    "    \"\"\"\n",
    "    Генерирует данные стратифицированным семплированием:\n",
    "    Возвращает датафрейм со значениями метрики и номерами страт пользователей\n",
    "    в контрольной и экспериментальной группах:\n",
    "    strat_to_param - словарь с параметрами страт\n",
    "    effect - размер эффекта\n",
    "    \"\"\"\n",
    "    control, pilot = [], []\n",
    "    \n",
    "    for strat, (_, n, mu, std) in strat_to_param.items():\n",
    "        control += [(x, strat) for x in stats.norm(mu, std).rvs(n)]\n",
    "        pilot += [(x, strat) for x in stats.norm(mu, std).rvs(n)]\n",
    "\n",
    "    control_df = pd.DataFrame(control, columns = ['metric', 'strat'])\n",
    "    control_df['is_treatment'] = 0\n",
    "    pilot_df = pd.DataFrame(pilot, columns = ['metric', 'strat'])\n",
    "    pilot_df['is_treatment'] = 1\n",
    "    \n",
    "    return pd.concat([control_df, pilot_df], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3b4b81",
   "metadata": {},
   "source": [
    "Базовый датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31710403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_base_df(strat_to_param,\n",
    "                     rho_lst = [0.9, 0.8, 0.7],\n",
    "                     treatment_effect = 0, \n",
    "                     sampling_way = 'random'):\n",
    "    '''\n",
    "    Генерирует базовый датасет:\n",
    "    strat_to_param - словарь с параметрами страт\n",
    "    rho_lst - список с коэффициентами корреляции для метрики и 3 ковариатов\n",
    "    sampling_way - способ семплирования (случайное / стратифицированное)\n",
    "    '''\n",
    "    \n",
    "    # sampling\n",
    "    \n",
    "    if sampling_way == 'random':\n",
    "        data = get_random_data(strat_to_param)\n",
    "    else:\n",
    "        data = get_stratified_data(strat_to_param)\n",
    "        \n",
    "    # covariates creation    \n",
    "    \n",
    "    data['metric_pre'] = rho_lst[0] * data['metric'] + \\\n",
    "        np.sqrt(1 - rho_lst[0]**2) * stats.norm(0, data['metric'].std(ddof = 1)).rvs(len(data))  \n",
    "    data['X1_pre'] = rho_lst[1] * data['metric'] + \\\n",
    "        np.sqrt(1 - rho_lst[1]**2) * stats.norm(0, data['metric'].std(ddof = 1)).rvs(len(data))  \n",
    "    data['X2_pre'] = rho_lst[2] * data['metric'] + \\\n",
    "        np.sqrt(1 - rho_lst[2]**2) * stats.norm(0, data['metric'].std(ddof = 1)).rvs(len(data))  \n",
    "    \n",
    "    data['metric_pre_pre'] = rho_lst[0] * data['metric_pre'] + \\\n",
    "        np.sqrt(1 - rho_lst[0]**2) * stats.norm(0, data['metric_pre'].std(ddof = 1)).rvs(len(data))  \n",
    "    data['X1_pre_pre'] = rho_lst[1] * data['metric_pre'] + \\\n",
    "        np.sqrt(1 - rho_lst[1]**2) * stats.norm(0, data['metric_pre'].std(ddof = 1)).rvs(len(data))  \n",
    "    data['X2_pre_pre'] = rho_lst[2] * data['metric_pre'] + \\\n",
    "        np.sqrt(1 - rho_lst[2]**2) * stats.norm(0, data['metric_pre'].std(ddof = 1)).rvs(len(data))  \n",
    "    \n",
    "    # treatment\n",
    "        \n",
    "    data.loc[data.is_treatment == 1, 'metric'] += treatment_effect\n",
    "    \n",
    "    # CUPED\n",
    "    \n",
    "    model = smf.ols('metric ~ metric_pre', data = data).fit()\n",
    "    data['metric_cuped'] = model.resid + data['metric'].mean()\n",
    "    \n",
    "    # CUMPED\n",
    "\n",
    "    model = smf.ols('metric ~ metric_pre + X1_pre + X2_pre', data = data).fit()\n",
    "    data['metric_cumped'] = model.resid + data['metric'].mean()\n",
    "    \n",
    "    # CUPAC\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = deepcopy(data[['metric_pre_pre', 'X1_pre_pre', 'X2_pre_pre']])\n",
    "    X_test = deepcopy(data[['metric_pre', 'X1_pre', 'X2_pre']])\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\", category = FutureWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category = ConvergenceWarning)\n",
    "    \n",
    "    model = RandomForestRegressor(n_estimators = 50,\n",
    "                                  max_depth = 7)\n",
    "\n",
    "    model.fit(X = X_train, y = data['metric_pre'])\n",
    "    data['metric_cupac'] = data['metric'] - model.predict(X_test) + data['metric'].mean()\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55080799",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = generate_base_df(strat_to_param,\n",
    "                        rho_lst = [0.5, 0.45, 0.4],\n",
    "                        treatment_effect = 0, \n",
    "                        sampling_way = 'random')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0e964f",
   "metadata": {},
   "source": [
    "Визуализация исходной и преобоазованных метрик:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44abb17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_metrics(n_iter):\n",
    "    '''\n",
    "    Визуализирует плотности распределения исходной и трансформированных метрик:\n",
    "    n_iter - число базовых датасетов, на основе которых строится график\n",
    "    '''\n",
    "    alpha = 0.3\n",
    "\n",
    "    data = generate_base_df(strat_to_param,\n",
    "                            rho_lst = [0.9, 0.8, 0.7],\n",
    "                            treatment_effect = 0, \n",
    "                            sampling_way = 'stratified')\n",
    "    \n",
    "    for _ in tqdm.tqdm(range(n_iter)):\n",
    "        data = pd.concat([data, \n",
    "                          generate_base_df(strat_to_param,\n",
    "                                           rho_lst = [0.9, 0.8, 0.7],\n",
    "                                           treatment_effect = 0, \n",
    "                                           sampling_way = 'random')],\n",
    "                        axis = 0)\n",
    "    metrics = ['metric_cuped', 'metric_cumped', 'metric_cupac']\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    ax_1 = axs[0]\n",
    "    ax_2 = axs[1]\n",
    "    \n",
    "    sns.kdeplot(x = 'metric', \n",
    "                data = data[data.is_treatment == 0], label = 'Initial', \n",
    "                fill = True, alpha = alpha, ax = ax_1)\n",
    "    for metric in metrics:\n",
    "        sns.kdeplot(x = metric, \n",
    "                    data = data[data.is_treatment == 0], label =  metric.replace('metric_', '').upper(), \n",
    "                    fill = True, alpha = alpha, ax = ax_1)\n",
    "    \n",
    "    ax_1.set_title('Control', fontsize = 15)\n",
    "    ax_1.legend(fontsize = 13)\n",
    "    ax_1.set_xlabel('Metric', fontsize = 15)\n",
    "    ax_1.set_ylabel('Density', fontsize = 15)\n",
    "\n",
    "    sns.kdeplot(x = 'metric',\n",
    "                data = data[data.is_treatment == 1], label = 'Initial', \n",
    "                fill = True, alpha = alpha, ax = ax_2)\n",
    "    for metric in metrics:\n",
    "        sns.kdeplot(x = metric, \n",
    "                    data = data[data.is_treatment == 1], label =  metric.replace('metric_', '').upper(), \n",
    "                    fill = True, alpha = alpha, ax = ax_2)\n",
    "        \n",
    "    ax_2.set_title('Pilot', fontsize = 15)\n",
    "    ax_2.set_ylabel('')\n",
    "    ax_2.set_yticklabels([])\n",
    "    ax_2.legend(fontsize = 13)\n",
    "    ax_2.set_xlabel('Metric', fontsize = 15)\n",
    "    ax_2.set_ylabel('Density', fontsize = 15)\n",
    "    \n",
    "    fig.suptitle('Initial and transformed metrics', fontsize = 17)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(16, 6))\n",
    "    sns.kdeplot(x = 'metric', \n",
    "                data = data, label = 'Initial', \n",
    "                fill = True, alpha = alpha, ax = axs)\n",
    "    for metric in metrics:\n",
    "        sns.kdeplot(x = metric, \n",
    "                    data = data, label = metric.replace('metric_', '').upper(), \n",
    "                    fill = True, alpha = alpha, ax = axs)\n",
    "        \n",
    "    axs.set_title('All users', fontsize = 15)\n",
    "    axs.legend(fontsize = 15)\n",
    "    axs.set_xlabel('Metric', fontsize = 15)\n",
    "    axs.set_ylabel('Density', fontsize = 15)\n",
    "    \n",
    "    plt.show();\n",
    "    \n",
    "visual_metrics(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c51379f",
   "metadata": {},
   "source": [
    "## Тесты"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5515d27c",
   "metadata": {},
   "source": [
    "### Снижение дисперсии"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2692d017",
   "metadata": {},
   "source": [
    "Корректируем тест Стьдента:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090bae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ttest(base_df, metric_name = 'metric', mean_calc_way = 'simple', return_deltas = False):\n",
    "    '''\n",
    "    Проверяет гипотезу о равенстве средних через t-test Стьюдента:\n",
    "    base_df -  базовый датасет для тестов\n",
    "    metric_name - метрка, на основе которой будет проводиться тест\n",
    "    mean_calc_way - способ подсчета среднего (средне арифметическое / среднее стратифицированное)\n",
    "    return_deltas - возвращать ли разницы средних (дельты)\n",
    "    '''\n",
    "    \n",
    "    control_df, test_df = base_df[base_df.is_treatment == 0], base_df[base_df.is_treatment == 1]\n",
    "    \n",
    "    if mean_calc_way == 'stratified':\n",
    "\n",
    "        control_mean = (control_df.groupby('strat')[metric_name].mean() * 0.5).sum()\n",
    "        test_mean = (test_df.groupby('strat')[metric_name].mean() * 0.5).sum()\n",
    "        control_var = (control_df.groupby('strat')[metric_name].var(ddof = 1) * 0.5).sum()\n",
    "        test_var = (test_df.groupby('strat')[metric_name].var(ddof = 1) * 0.5).sum()\n",
    "\n",
    "        delta = test_mean - control_mean\n",
    "        std = (control_var / len(control_df) + test_var / len(test_df)) ** 0.5\n",
    "        t = delta / std\n",
    "    \n",
    "        pvalue = 2 * (1 - stats.t(len(control_df) + len(test_df)).cdf(np.abs(t)))\n",
    "    \n",
    "        if return_deltas == True:\n",
    "            return (pvalue, delta)\n",
    "        else:\n",
    "            return pvalue\n",
    "    else:\n",
    "        \n",
    "        delta = test_df[metric_name].mean() - control_df[metric_name].mean()\n",
    "        _, pvalue = stats.ttest_ind(control_df[metric_name], test_df[metric_name])\n",
    "        \n",
    "        if return_deltas == True:\n",
    "            return (pvalue, delta)\n",
    "        else:\n",
    "            return pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32399fca",
   "metadata": {},
   "source": [
    "\"Оси\", в которых будет производиться сравнение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cda900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обычный тест, стратификация, постстратификация\n",
    "methods = {\n",
    "    \n",
    "    'random_sampling_simple_mean': {'sampling_way': 'random',\n",
    "                                    'mean_calc_way': 'simple'},\n",
    "    'random_sampling_stratified_mean': {'sampling_way': 'random',\n",
    "                                        'mean_calc_way': 'stratified'},\n",
    "    'stratified_sampling_stratified_mean': {'sampling_way': 'stratified',\n",
    "                                            'mean_calc_way': 'stratified'} \n",
    "}\n",
    "\n",
    "# Исходная метрика, CU(M)PED / CUPAC - трансформированные метрики\n",
    "techniques = {'initial': 'metric', 'cuped': 'metric_cuped', 'cumped': 'metric_cumped', 'cupac': 'metric_cupac'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c57c937",
   "metadata": {},
   "source": [
    "Автоматизируем подсчет ошибки I рода:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f75e342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_first_type_errors(methods, techniques, rho_lst, n_tests):\n",
    "    '''\n",
    "    Считает ошибку I рода:\n",
    "    methods - словарь с методами. Содержит способ семплирования и способ подсчета среднего\n",
    "    techniques - словарь с техниками. Содержит короткое название и метрики (техники), \n",
    "    для которых необходимо подсчитать ошибку I рода\n",
    "    rho_lst - массив с корреляциями метрики и ковариатов для генерации базового датасета\n",
    "    на каждой итерации\n",
    "    n_tests - количество итераций, на основе которых будут посчитаны ошибки I рода\n",
    "    '''\n",
    "    \n",
    "    fte_result = pd.DataFrame()\n",
    "    \n",
    "    for method in methods:\n",
    "        sampling_way = methods[method]['sampling_way']\n",
    "        mean_calc_way = methods[method]['mean_calc_way']\n",
    "        \n",
    "        first_type_errors_dct = {technique: [] for technique in techniques}\n",
    "        \n",
    "        for _ in tqdm.tqdm(range(n_tests), leave = None):\n",
    "            \n",
    "            base_df = generate_base_df(strat_to_param,\n",
    "                      rho_lst = rho_lst, # [0.5, 0.45, 0.4], # [0.9, 0.8, 0.7],\n",
    "                      treatment_effect = 0, \n",
    "                      sampling_way = sampling_way)\n",
    "            \n",
    "            for technique in techniques:\n",
    "                pvalue = calc_ttest(base_df, \n",
    "                                    metric_name = techniques[technique],  \n",
    "                                    mean_calc_way = mean_calc_way,\n",
    "                                    return_deltas = False)\n",
    "                first_type_errors_dct[technique].append(pvalue < alpha)\n",
    "            \n",
    "        data = {f'{method}': {technique: np.array(first_type_errors_dct[technique]).mean() for technique in techniques}}\n",
    "        data = pd.DataFrame.from_dict(data, orient = 'columns')\n",
    "        \n",
    "        if len(fte_result) == 0:\n",
    "            fte_result = deepcopy(data)\n",
    "        else:\n",
    "            fte_result = fte_result.join(data, how = 'left')\n",
    "    \n",
    "    return fte_result\n",
    "\n",
    "\n",
    "rho_lst = [0.7, 0.65, 0.6]\n",
    "n_tests = 10\n",
    "fte_result = calc_first_type_errors(methods, techniques, rho_lst, n_tests)\n",
    "fte_result.to_csv('./csv_saves/fte_result.csv')\n",
    "fte_result # pd.read_csv('./csv_saves/fte_result.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d2fb34",
   "metadata": {},
   "source": [
    "Автоматизируем подсчет ошибки II рода и разниц средних (дельт):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dff6065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_second_type_errors(methods, techniques, rho_lst, n_tests):\n",
    "    '''\n",
    "    Считает ошибку II рода, а также разницы средних и их дисперсию:\n",
    "    methods - словарь с методами. Содержит способ семплирования и способ подсчета среднего\n",
    "    techniques - словарь с техниками. Содержит короткое название и метрики (техники), \n",
    "    для которых необходимо подсчитать ошибку II рода\n",
    "    rho_lst - массив с корреляциями метрики и ковариатов для генерации базового датасета\n",
    "    на каждой итерации\n",
    "    n_tests - количество итераций, на основе которых будут посчитаны ошибки II рода\n",
    "    '''\n",
    "    \n",
    "    ste_result = pd.DataFrame()\n",
    "    deltas_result = pd.DataFrame()\n",
    "    deltas_vis = dict()\n",
    "    metrics_vis = dict()\n",
    "    \n",
    "    for method in methods:\n",
    "        sampling_way = methods[method]['sampling_way']\n",
    "        mean_calc_way = methods[method]['mean_calc_way']\n",
    "        \n",
    "        second_type_errors_dct = {technique: [] for technique in techniques}\n",
    "        deltas_dct = {technique: [] for technique in techniques}\n",
    "        metrics_dct = {technique: [] for technique in techniques}\n",
    "        \n",
    "        for _ in tqdm.tqdm(range(n_tests), leave = None):\n",
    "\n",
    "            base_df = generate_base_df(strat_to_param,\n",
    "                      rho_lst = rho_lst, # [0.5, 0.45, 0.4], # [0.9, 0.8, 0.7],\n",
    "                      treatment_effect = 100, \n",
    "                      sampling_way = sampling_way)\n",
    "            \n",
    "            for technique in techniques:\n",
    "                pvalue, delta = calc_ttest(base_df, \n",
    "                                           metric_name = techniques[technique],  \n",
    "                                           mean_calc_way = mean_calc_way,\n",
    "                                           return_deltas = True)\n",
    "                second_type_errors_dct[technique].append(pvalue >= alpha)\n",
    "                deltas_dct[technique].append(delta)\n",
    "                metrics_dct[technique] += base_df[techniques[technique]].values.tolist()\n",
    "            \n",
    "        ste_data = {f'{method}': {technique: np.array(second_type_errors_dct[technique]).mean() for technique in techniques}}\n",
    "        ste_data = pd.DataFrame.from_dict(ste_data, orient = 'columns')\n",
    "        \n",
    "        deltas_data = {f'{method}': {technique: np.array(deltas_dct[technique]).var(ddof = 1) for technique in techniques}}\n",
    "        deltas_data = pd.DataFrame.from_dict(deltas_data, orient = 'columns')\n",
    "        deltas_vis[f'{method}'] = {technique: np.array(deltas_dct[technique]) for technique in techniques}\n",
    "        \n",
    "        metrics_dct = {technique: np.array(metrics_dct[technique]) for technique in techniques}\n",
    "        metrics_vis[f'{method}'] = metrics_dct\n",
    "        \n",
    "        if len(ste_result) == 0:\n",
    "            ste_result = deepcopy(ste_data)\n",
    "            deltas_result = deepcopy(deltas_data)\n",
    "        else:\n",
    "            ste_result = ste_result.join(ste_data, how = 'left')\n",
    "            deltas_result = deltas_result.join(deltas_data, how = 'left')\n",
    "    \n",
    "    return ste_result, deltas_result, deltas_vis, metrics_vis\n",
    "\n",
    "\n",
    "rho_lst = [0.35, 0.3, 0.25]\n",
    "n_tests = 10\n",
    "ste_result, deltas_result, deltas_vis, metrics_vis = calc_second_type_errors(methods, techniques, rho_lst, n_tests)\n",
    "ste_result.to_csv('./csv_saves/ste_result.csv')\n",
    "deltas_result.to_csv('./csv_saves/deltas_result.csv')\n",
    "ste_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22481b14",
   "metadata": {},
   "source": [
    "Визуализация скоращения дисперсии разницы средних:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d6e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_deltas_var(deltas_result, rho_lst):\n",
    "    '''\n",
    "    Визуализирует скоращение дисперсии разницы средних:\n",
    "    deltas_result - pd.DataFrame с дисперсиями разницы средних по различным методам\n",
    "    rho_lst - массив с корреляциями метрики и ковариатов, на основе которого был сгенерирован\n",
    "    базовый датасет и посчитаны дисперсии разницы средних\n",
    "    '''\n",
    "    data = deepcopy(deltas_result)\n",
    "\n",
    "    columns = [column.replace('_', ' ').replace('ing ', 'ing / ') for column in data.columns]\n",
    "\n",
    "    rows = list(data.index)\n",
    "\n",
    "    data = data.to_numpy()\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    xpos, ypos = np.meshgrid(range(data.shape[1]), range(data.shape[0]))\n",
    "\n",
    "    xpos = xpos.flatten()\n",
    "    ypos = ypos.flatten()\n",
    "    zpos = np.zeros(len(xpos))\n",
    "    dx = dy = 0.8\n",
    "    dz = data.flatten()\n",
    "\n",
    "    colors = 1 - dz / dz.max()  \n",
    "    colors = plt.cm.RdYlBu(colors)  \n",
    "\n",
    "    ax.bar3d(xpos, ypos, zpos, dx, dy, dz,  color=colors)\n",
    "    \n",
    "    initial_metric_index = list(rows).index('initial')\n",
    "    percent_decrease = (data[initial_metric_index][0] - data) / data[initial_metric_index][0] * 100\n",
    "    for i, j, z, decrease in zip(xpos, ypos, dz, percent_decrease.flatten()):\n",
    "        if decrease != 0:\n",
    "            ax.text(i + dx/2, j + dy/2, z + dz.max() * -0.03, f'{decrease:.2f}%', ha='center', color='black')\n",
    "\n",
    "    ax.set_xticks([i + 0.5 for i in range(data.shape[1])], minor = True)\n",
    "    ax.set_yticks([i + 0.5 for i in range(data.shape[0])], minor = True)\n",
    "    ax.set_xticks(np.arange(-0.5, data.shape[1] - 0.5, 1), minor = False)\n",
    "    ax.set_yticks(range(1, data.shape[0] + 1), minor = False, labels = rows)\n",
    "    ax.set_xticklabels(columns, rotation = 15, fontweight='bold')\n",
    "    ax.set_yticklabels(rows, fontweight='bold', minor = False)\n",
    "    ax.tick_params(axis='both', pad=10)\n",
    "\n",
    "    ax.set_title(f'Deltas variance reduction \\n Corr(Y, X$_i$) = {rho_lst}', fontsize = 14)\n",
    "    \n",
    "    plt.savefig('./plot_saves/deltas_variance.png')\n",
    "    plt.show();\n",
    "    \n",
    "visual_deltas_var(deltas_result, rho_lst)\n",
    "# Image(filename='./plot_saves/deltas_variance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1141c7b",
   "metadata": {},
   "source": [
    "То же самое, только теперь несколько наборов корреляций на одном графике:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6ea946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_several_deltas_var(deltas_results, rho_lsts):\n",
    "    '''\n",
    "    Визуализирует скоращение дисперсии разницы средних по списку массивов корреляций:\n",
    "    deltas_result - список с pd.DataFrame, которые содеражт дисперсиии разницы средних по различным методам\n",
    "    rho_lst - список массивов с корреляциями метрики и ковариатов, на основе которых были сгенерированы\n",
    "    базовые датасеты и посчитаны дисперсии разницы средних\n",
    "    '''\n",
    "    \n",
    "    fig = plt.figure(figsize=(17, 15))\n",
    "\n",
    "    for idx in range(1, len(rho_lsts)+1):\n",
    "\n",
    "        ax = fig.add_subplot(2, 2, idx, projection='3d')\n",
    "\n",
    "\n",
    "        data = deepcopy(deltas_results[idx-1])\n",
    "\n",
    "        columns = [column.replace('_', ' ').replace('ing ', 'ing / ') for column in data.columns]\n",
    "\n",
    "        rows = list(data.index)\n",
    "\n",
    "        data = data.to_numpy()\n",
    "\n",
    "        xpos, ypos = np.meshgrid(range(data.shape[1]), range(data.shape[0]))\n",
    "\n",
    "        xpos = xpos.flatten()\n",
    "        ypos = ypos.flatten()\n",
    "        zpos = np.zeros(len(xpos))\n",
    "        dx = dy = 0.8\n",
    "        dz = data.flatten()\n",
    "\n",
    "        colors = 1 - dz / dz.max()  \n",
    "        colors = plt.cm.RdYlBu(colors)  \n",
    "\n",
    "        ax.bar3d(xpos, ypos, zpos, dx, dy, dz,  color=colors)\n",
    "\n",
    "        initial_metric_index = list(rows).index('initial')\n",
    "        percent_decrease = (data[initial_metric_index][0] - data) / data[initial_metric_index][0] * 100\n",
    "        for i, j, z, decrease in zip(xpos, ypos, dz, percent_decrease.flatten()):\n",
    "            if decrease != 0:\n",
    "                ax.text(i + dx/2, j + dy/2, z + dz.max() * -0.03, f'{decrease:.2f}%', ha='center', color='black')\n",
    "\n",
    "        ax.set_xticks([i + 0.5 for i in range(data.shape[1])], minor = True)\n",
    "        ax.set_yticks([i + 0.5 for i in range(data.shape[0])], minor = True)\n",
    "        ax.set_xticks(np.arange(-0.5, data.shape[1] - 0.5, 1), minor = False)\n",
    "        ax.set_yticks(range(1, data.shape[0] + 1), minor = False, labels = rows)\n",
    "        ax.set_xticklabels(columns, rotation = 15, fontweight='bold', fontsize = 14)\n",
    "        ax.set_yticklabels(rows, fontweight='bold', minor = False, fontsize = 14)\n",
    "        ax.tick_params(axis='y', pad=10)\n",
    "        ax.tick_params(axis='x', pad=15)\n",
    "        \n",
    "        ax.set_title(f'Corr(Y, X$_i$) = {rho_lsts[idx-1]}', fontsize = 16)\n",
    "            \n",
    "    fig.suptitle('Deltas variance reduction', fontsize=20)\n",
    "    plt.subplots_adjust(top=0.93)\n",
    "    plt.show();\n",
    "    \n",
    "    \n",
    "rho_lsts = [[0.35, 0.3, 0.25], [0.5, 0.45, 0.4], [0.65, 0.6, 0.55], [0.8, 0.75, 0.7]]\n",
    "n_tests = 25000\n",
    "deltas_results = []\n",
    "for rho_lst in tqdm.tqdm(rho_lsts):\n",
    "    _, deltas_result, _, _ = calc_second_type_errors(methods, techniques, rho_lst, n_tests)\n",
    "    deltas_results.append(deltas_result)\n",
    "\n",
    "visual_several_deltas_var(deltas_results, rho_lsts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41030d4",
   "metadata": {},
   "source": [
    "Визуализация плотостей распределения разностей средних для разных методов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6db5f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_deltas(deltas_vis):\n",
    "    '''\n",
    "    Визуализирует плотности распределения разницы средних для различных методов сокращения дисперсии:\n",
    "    deltas_vis - словарь, который содержит методы и массивы с разницами средних\n",
    "    '''\n",
    "    \n",
    "    fig, axs = plt.subplots(len(deltas_vis), 1, figsize=(7, 17))    \n",
    "    for n, method in enumerate(deltas_vis):\n",
    "        for c, technique in enumerate(deltas_vis[method]):\n",
    "            sns.kdeplot(deltas_vis[method][technique], \n",
    "                        label = f'{technique}',\n",
    "                        color = sns.color_palette('bright')[c+1],\n",
    "                        fill = True, \n",
    "                        alpha = 0.2, \n",
    "                        ax = axs[n])\n",
    "            axs[n].set_title(f\"{method.replace('_', ' ').replace('ing ', 'ing / ')}\")\n",
    "            axs[n].set_ylabel('')\n",
    "            axs[n].legend()   \n",
    "            \n",
    "    plt.subplots_adjust(top=0.95)\n",
    "    plt.subplots_adjust(hspace=0.3)\n",
    "    plt.suptitle('Deltas density', fontsize=16)\n",
    "    sns.set_style(\"white\")\n",
    "    sns.despine()\n",
    "    plt.savefig('./plot_saves/deltas_density.png')\n",
    "    plt.show();\n",
    "    \n",
    "visual_deltas(deltas_vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c3f00b",
   "metadata": {},
   "source": [
    "### Ошибки I и II рода"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e7f88a",
   "metadata": {},
   "source": [
    "Тестируемые значения коэффицитов корреляции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bd3361",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_list = [0.05, 0, -0.05]\n",
    "result_lists = [start_list]\n",
    "\n",
    "while start_list[0] < 0.95:\n",
    "    start_list = [x + 0.05 for x in start_list]\n",
    "    result_lists.append(start_list)\n",
    "\n",
    "result_lists = [lst for lst in result_lists if lst != [0.05, 0, -0.05]]\n",
    "\n",
    "rho_lsts = result_lists\n",
    "n_tests = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbec99b3",
   "metadata": {},
   "source": [
    "Визуализация ошибок I и II рода в зависимости от корреляции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3885ac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_errors(methods, techniques, rho_lsts, n_tests):\n",
    "    '''\n",
    "    Визуализирует значения ошибок I и II в зависимости от корреляции:\n",
    "    methods - словарь с методами. Содержит способ семплирования и способ подсчета среднего\n",
    "    techniques - словарь с техниками. Содержит короткое название и метрики (техники), \n",
    "    для которых необходимо подсчитать ошибку II рода\n",
    "    rho_lsts - список массивов с корреляциями метрики и ковариатов, для которых нужно вычислить ошибку I и II рода\n",
    "    n_tests - количество итераций, на основе которых будут посчитаны ошибки I и II рода\n",
    "    '''\n",
    "    fte_data = pd.DataFrame()\n",
    "    ste_data = pd.DataFrame()\n",
    "    \n",
    "    for rho_lst in tqdm.tqdm(rho_lsts, leave = None):\n",
    "        \n",
    "        fte_result = calc_first_type_errors(methods, techniques, rho_lst, n_tests)\n",
    "        ste_result, _, _, _ = calc_second_type_errors(methods, techniques, rho_lst, n_tests)\n",
    "        \n",
    "        fte_data = pd.concat([fte_data, fte_result], axis=0)\n",
    "        ste_data = pd.concat([ste_data, ste_result], axis=0)\n",
    "        \n",
    "    fte_dct = fte_data.groupby(fte_data.index).agg(list).to_dict(orient='dict')\n",
    "    ste_dct = ste_data.groupby(ste_data.index).agg(list).to_dict(orient='dict')\n",
    "    \n",
    "    corr_lst = [x[0] for x in rho_lsts]\n",
    "\n",
    "    titles = ['<b>' + key.replace('_', ' ').replace('ing ', 'ing / ') + '</b>'  for key in list(fte_dct.keys())]\n",
    "    fig = make_subplots(rows=len(fte_dct.keys()), cols = 1, subplot_titles=titles,  vertical_spacing=0.1)\n",
    "\n",
    "    tech = list(fte_dct[list(fte_dct.keys())[0]].keys())\n",
    "    colors = {tech: px.colors.qualitative.Plotly[i] for i, tech in enumerate(tech)}\n",
    "\n",
    "    for n, method in enumerate(fte_dct):\n",
    "        for technique in fte_dct[method]:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=corr_lst,\n",
    "                y=fte_dct[method][technique],\n",
    "                legendgroup=f\"group{n}_fte\",  \n",
    "                legendgrouptitle_text=\"I type error\",\n",
    "                showlegend = True if n == 0 else False,\n",
    "                name=technique,\n",
    "                mode=\"lines\",\n",
    "                line=dict(color=colors[technique], dash=\"dot\", width=1)\n",
    "            ), row=n+1, col=1)\n",
    "\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=corr_lst,\n",
    "                y=ste_dct[method][technique],\n",
    "                legendgroup=f\"group{n}_ste\",  \n",
    "                legendgrouptitle_text=\"II type error\",\n",
    "                showlegend = True if n == 0 else False,\n",
    "                name=technique,\n",
    "                mode=\"lines\",\n",
    "                line=dict(color=colors[technique], width=1)\n",
    "            ), row=n+1, col=1)\n",
    "            fig.update_xaxes(title_text=\"Corr(Y, X<sub>1</sub>)\", row=n+1, col=1, showline=True, linewidth=1, linecolor='black')\n",
    "            fig.update_yaxes(title_text=\"Error\", row=n+1, col=1, showline=True, linewidth=1, linecolor='black')\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"I / II type error dependency on correlation\",\n",
    "        title_x=0.5,\n",
    "        plot_bgcolor=\"white\",\n",
    "        width=600, height=1000)\n",
    "\n",
    "    fig.show();\n",
    "    \n",
    "    return fte_dct, ste_dct\n",
    "\n",
    "fte_dct, ste_dct = visual_errors(methods, techniques, rho_lsts, n_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9fe8f5",
   "metadata": {},
   "source": [
    "### Эксперименты с CUPAC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fd0bd6",
   "metadata": {},
   "source": [
    "Преобразуем немного функцию генерации базового датасета по части CUPAC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521900fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_base_df(strat_to_param,\n",
    "                     rho_lst = [0.9, 0.8, 0.7],\n",
    "                     treatment_effect = 0, \n",
    "                     sampling_way = 'random'):\n",
    "    '''\n",
    "    Генерирует базовый датасет с акцентом на модель, лежащую в основе CUPAC:\n",
    "    strat_to_param - словарь с параметрами страт\n",
    "    rho_lst - список с коэффициентами корреляции для метрики и 3 ковариатов\n",
    "    sampling_way - способ семплирования (случайное / стратифицированное)\n",
    "    '''\n",
    "    \n",
    "    # sampling\n",
    "    \n",
    "    if sampling_way == 'random':\n",
    "        data = get_random_data(strat_to_param)\n",
    "    else:\n",
    "        data = get_stratified_data(strat_to_param)\n",
    "        \n",
    "    # covariates creation    \n",
    "    \n",
    "    data['metric_pre'] = rho_lst[0] * data['metric'] + \\\n",
    "        np.sqrt(1 - rho_lst[0]**2) * stats.norm(0, data['metric'].std(ddof = 1)).rvs(len(data))  \n",
    "    data['X1_pre'] = rho_lst[1] * data['metric'] + \\\n",
    "        np.sqrt(1 - rho_lst[1]**2) * stats.norm(0, data['metric'].std(ddof = 1)).rvs(len(data))  \n",
    "    data['X2_pre'] = rho_lst[2] * data['metric'] + \\\n",
    "        np.sqrt(1 - rho_lst[2]**2) * stats.norm(0, data['metric'].std(ddof = 1)).rvs(len(data))  \n",
    "    \n",
    "    data['metric_pre_pre'] = rho_lst[0] * data['metric_pre'] + \\\n",
    "        np.sqrt(1 - rho_lst[0]**2) * stats.norm(0, data['metric_pre'].std(ddof = 1)).rvs(len(data))  \n",
    "    data['X1_pre_pre'] = rho_lst[1] * data['metric_pre'] + \\\n",
    "        np.sqrt(1 - rho_lst[1]**2) * stats.norm(0, data['metric_pre'].std(ddof = 1)).rvs(len(data))  \n",
    "    data['X2_pre_pre'] = rho_lst[2] * data['metric_pre'] + \\\n",
    "        np.sqrt(1 - rho_lst[2]**2) * stats.norm(0, data['metric_pre'].std(ddof = 1)).rvs(len(data))  \n",
    "    \n",
    "    # treatment\n",
    "        \n",
    "    data.loc[data.is_treatment == 1, 'metric'] += treatment_effect\n",
    "        \n",
    "    # CUMPED\n",
    "\n",
    "    model = smf.ols('metric ~ metric_pre + X1_pre + X2_pre', data = data).fit()\n",
    "    data['metric_cumped'] = model.resid + data['metric'].mean()\n",
    "        \n",
    "    scaler = StandardScaler()\n",
    "    X_train = deepcopy(data[['metric_pre_pre', 'X1_pre_pre', 'X2_pre_pre']])\n",
    "    X_test = deepcopy(data[['metric_pre', 'X1_pre', 'X2_pre']])\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # CUPAC_RF\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\", category = FutureWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category = ConvergenceWarning)\n",
    "    \n",
    "    model = RandomForestRegressor(n_estimators = 50, \n",
    "                                  max_depth = 7)\n",
    "    \n",
    "    model.fit(X = X_train, y = data['metric_pre'])\n",
    "    data['metric_cupac_rf'] = data['metric'] - model.predict(X_test) + data['metric'].mean()\n",
    "    \n",
    "    # CUPAC_GB\n",
    "    \n",
    "    model = GradientBoostingRegressor(n_estimators = 50,\n",
    "                                      criterion = 'squared_error',\n",
    "                                      max_depth = 3,\n",
    "                                      subsample = 0.6,\n",
    "                                      learning_rate = 0.07)\n",
    "    \n",
    "    model.fit(X = X_train, y = data['metric_pre'])\n",
    "    data['metric_cupac_gb'] = data['metric'] - model.predict(X_test) + data['metric'].mean()\n",
    "    \n",
    "    # CUPAC_MLP\n",
    "    \n",
    "    model =  MLPRegressor(hidden_layer_sizes = (30, 15),\n",
    "                          solver = 'sgd', \n",
    "                          learning_rate_init = 0.000001,\n",
    "                          learning_rate = 'invscaling',\n",
    "                          power_t = 0.1,\n",
    "                          max_iter = 300,\n",
    "                          alpha = 0.1)\n",
    "    \n",
    "    model.fit(X = X_train, y = data['metric_pre'])\n",
    "    data['metric_cupac_mlp'] = data['metric'] - model.predict(X_test) + data['metric'].mean()\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7ae318",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = generate_base_df(strat_to_param,\n",
    "                        rho_lst = [0.5, 0.45, 0.4],\n",
    "                        treatment_effect = 0, \n",
    "                        sampling_way = 'stratified')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166f2f10",
   "metadata": {},
   "source": [
    "Новые \"оси\", в которых будет производиться сравнение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa1e325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обычный тест, стратификация, постстратификация\n",
    "methods = {\n",
    "    \n",
    "    'random_sampling_simple_mean': {'sampling_way': 'random',\n",
    "                                    'mean_calc_way': 'simple'},\n",
    "    'random_sampling_stratified_mean': {'sampling_way': 'random',\n",
    "                                        'mean_calc_way': 'stratified'},\n",
    "    'stratified_sampling_stratified_mean': {'sampling_way': 'stratified',\n",
    "                                            'mean_calc_way': 'stratified'} \n",
    "}\n",
    "\n",
    "# Исходная метрика, CUMPED / CUPAC (RF, GBDT, MLP) - трансформированные метрики\n",
    "techniques = {'initial': 'metric',\n",
    "              'cumped': 'metric_cumped',\n",
    "              'cupac_rf': 'metric_cupac_rf', \n",
    "              'cupac_gb': 'metric_cupac_gb', \n",
    "              'cupac_mlp': 'metric_cupac_mlp'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcebe79",
   "metadata": {},
   "source": [
    "#### Снижение дисперсии"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e07f2d7",
   "metadata": {},
   "source": [
    "Визуализация скоращения дисперсии разницы средних:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731313f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rho_lst = [0.65, 0.6, 0.55]\n",
    "n_tests = 25000\n",
    "_, deltas_result, deltas_vis, _ = calc_second_type_errors(methods, techniques, rho_lst, n_tests)\n",
    "\n",
    "new_index = ['cumped', 'cupac_mlp', 'cupac_gb', 'cupac_rf', 'initial']\n",
    "deltas_result = deltas_result.reindex(new_index)\n",
    "visual_deltas_var(deltas_result, rho_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2127640",
   "metadata": {},
   "source": [
    "Плотности распределения разностей средних для разных вариантов CUPAC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126e86bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_deltas(deltas_vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0051390f",
   "metadata": {},
   "source": [
    "#### Ошибки I и II рода"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421a4626",
   "metadata": {},
   "source": [
    "Визуализация значений ошибок I и II рода в зависимости от корреляции и модели, лежащей в основе CUPAC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf01b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_list = [0.05, 0, -0.05]\n",
    "result_lists = [start_list]\n",
    "\n",
    "while start_list[0] < 0.95:\n",
    "    start_list = [x + 0.05 for x in start_list]\n",
    "    result_lists.append(start_list)\n",
    "\n",
    "result_lists = [lst for lst in result_lists if lst != [0.05, 0, -0.05]]\n",
    "\n",
    "rho_lsts = result_lists\n",
    "n_tests = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceae2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fte_dct, ste_dct = visual_errors(methods, techniques, rho_lsts, n_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74ae525",
   "metadata": {},
   "source": [
    "Материалы по теме исследования:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3569ba",
   "metadata": {},
   "source": [
    "* https://www.microsoft.com/en-us/research/group/experimentation-platform-exp/articles/deep-dive-into-variance-reduction/\n",
    "\n",
    "* https://towardsdatascience.com/how-to-double-a-b-testing-speed-with-cuped-f80460825a90\n",
    "    \n",
    "* https://research.facebook.com/blog/2020/10/increasing-the-sensitivity-of-a-b-tests-by-utilizing-the-variance-estimates-of-experimental-units/\n",
    "    \n",
    "* https://craft.faire.com/how-to-speed-up-your-a-b-test-outlier-capping-and-cuped-8c9df21c76b\n",
    "    \n",
    "* https://booking.ai/how-booking-com-increases-the-power-of-online-experiments-with-cuped-995d186fff1d\n",
    "    \n",
    "* https://habr.com/ru/companies/yandex/articles/497804/\n",
    "    \n",
    "* https://doordash.engineering/2020/06/08/improving-experimental-power-through-control-using-predictions-as-covariate-cupac/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "479.473px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
